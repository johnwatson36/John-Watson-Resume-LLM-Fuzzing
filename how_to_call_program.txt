Guide to Running PyTorch API Generation and Testing

    1. Run the main script to generate APIs:
    Command: `python main.py`
    - This script uses calculates and then uses the optimal temperature value to generate PyTorch API calls.
    - Output: The generated API calls are saved to `apis.csv`.
    - API example:
            - api_name,parameters
            - torch.nn.functional.gumbel_softmax,"logits, tau, hard, eps, dim"
            - torch.utils.data.dataloader.default_collate,batch


Metrics Used for Deciding Temperature Value:
The quality of generated API calls is determined using the following metrics:

    1. Number of Parameters:
    - APIs with more parameters are often more complex and useful for testing.
    - Metric: APIs with more than 3 parameters receive a higher score, promoting the generation of sophisticated API calls.

    2. Uniqueness of Parameter Names:
    - Ensures that parameters serve distinct purposes and contribute to a well-formed API.
    - Metric: If all parameter names are unique, the API gets a higher score.

    3. Validity of API Name:
    - Confirms that the API call is valid by checking if it starts with 'torch.'.
    - Metric: If the API name is valid (begins with 'torch.'), it receives a higher score.

    How These Metrics Affect Temperature:

    - High Average Score:
    - If the generated APIs score highly, the temperature decreases, focusing on generating relevant, high-quality API calls.
    
    - Low Average Score:
    - If the score is low, the temperature increases to introduce more diversity and creativity in generating APIs.

    - Moderate Average Score:
    - The temperature remains unchanged, maintaining a balance between creativity and relevance.

    Summary of Metrics:
    - Number of Parameters: More parameters indicate more complex and useful APIs.
    - Uniqueness of Parameter Names: Ensures that each parameter serves a distinct purpose.
    - Validity of API Name: Confirms that the API call is relevant and from the PyTorch library.
    By adjusting the temperature based on these metrics, the model dynamically improves the quality of its generated APIs.


Testing the APIs Using `pytorch_bug_analyzer.py`

    Once the APIs are generated and saved to `apis.csv`, you can test the API calls with the PyTorch bug analyzer. This step checks if the generated API calls are valid and if they can be executed successfully with the provided parameters.

    Steps to run the PyTorch Bug Analyzer:
    1. Command: `python pytorch_bug_analyzer.py`
    - This script reads the generated APIs from `apis.csv`, generates test programs, and checks if the parameters work correctly when executed.
    - It evaluates the generated APIs on both CPU and GPU, checking for any errors or inconsistencies.

    2. Outputs and Testing:
    - The script generates test programs for each API and parameter set.
    - It checks if the API works correctly with the provided parameters on both CPU and GPU.
    - If an API call fails, it prints out the error and saves the buggy program.

    Example Output:
    - If an API fails due to syntax errors or incorrect parameter values, the script outputs:
        - API call failed: torch.nn.functional.gumbel_softmax,"logits, tau, hard, eps, dim" Error: name 'logits' is not defined
        - This indicates that the API could not run because `logits` (a parameter) was not properly defined.

    - If the API call succeeds, it outputs:
        Buggy Program Handling:
        - If a program is found to be buggy (e.g., it produces different results on CPU and GPU), it is saved to a `buggy-programs` folder with details of the bug.
        - The `pytorch_bug_analyzer.py` script will also print detailed information about the bug, including the CPU and GPU output for comparison.


Final Statistics and Output:

    At the end of the bug analysis, the script saves important statistics to `statistics.csv`:
    - Total Programs Generated
    - Valid Programs
    - Buggy Programs
    - API Coverage
    - Program Recalls
    - Total Runtime

    To view the statistics, check the final lines of the `statistics.csv` file or the terminal output.

    The results output to the terminal is also saved to a results.txt file


Summary of Steps:

    1. Generate API calls with `main.py`:
    - Output: `apis.csv`
    
    2. Analyze and test the generated APIs using `pytorch_bug_analyzer.py`:
    - Output: Buggy programs (if found) saved in `buggy-programs/`
    - Final statistics are saved in `statistics.csv`.
